{"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"sourceType":"datasetVersion","sourceId":2782108,"datasetId":1649650,"databundleVersionId":2827886},{"sourceType":"datasetVersion","sourceId":7779672,"datasetId":3951500,"databundleVersionId":7881348},{"sourceType":"kernelVersion","sourceId":181331759}],"dockerImageVersionId":30636,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"+ Classificação de doenças (Rusty, Miner, Phoma) em folhas de café.\n+ Rotina de Treinamento.\n+ Fabio Cardoso.","metadata":{}},{"cell_type":"code","source":"# Parâmetros\n\nsmp_sz           = 24      #Tamanho da amostra (de preferencia multiplo de 4) para cada epoch de treinamento.\nqt_categs        = 3       #Quantidade distinta de labels/categorias.\nqt_threads       = 4       #Paralelização\nimg_sq_size      = 384     #Tamanho para resizing das imagens.\nepochs_traininig = 50      #Quantidade de epochs de treinamento.\nfile_ext         = '.jpg'  #Tipo de imagem.\nuse_base_model   = True    #Se faz uso de modelo pré-treinado.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Imports\n\n#Geral\nimport os, gc\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.ndimage import rotate\nfrom sklearn.metrics import log_loss\n\n#images and tensorflow \nfrom PIL import Image\nImage.MAX_IMAGE_PIXELS = None\nimport tensorflow as tf\n\n#Processamento paralelo\nimport zlib\nimport pickle\nimport joblib\nfrom multiprocessing import Pool\n\n#Libs customizadas\nimport util0 #Preparação dos metadados\nimport util1f #Augmentation e preparação das imagens\n\n# Keras\nimport keras\nfrom keras import regularizers\nfrom keras.layers import Input\nfrom keras.layers import Dense\nfrom keras.models import Model\nfrom keras.models import Sequential\nfrom keras.models import load_model\nfrom keras.layers import Dropout\nfrom keras.callbacks import EarlyStopping\nfrom keras.layers import BatchNormalization\n\n# Tensorflow\nimport tensorflow as tf\nfrom keras.regularizers import l2\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.layers import Attention\nfrom tensorflow.keras import datasets, layers, models, initializers\n\nnp.random.seed(np.random.randint(999999999))\ntf.random.set_seed(np.random.randint(999999999))","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Paths\n\npath_data  = '/kaggle/input/coffee-leaf-diseases/'\npath_out   = '/kaggle/working/'\npath_model = '/kaggle/input/coffee-leafs-classification-ok/'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Metadados - processamento paralelizado\n\nimgnm_field = 'ImgName'\nimdir_field = 'ImgDir'\nlbl01_field =  None\n\nqt_parts = 4 #Partes em paralelo (4 processadores)\n    \ndf_meta_ini = pd.read_csv(path_data + 'train_classes.csv') #arquivo com labels\nqt_per_part_train = int(df_meta_ini.shape[0] / qt_parts)\nend_train = 0\n\n#Popula parâmetros para chamadas em paralelo\nparms = []\nfor i in range(qt_parts):\n        ini_train = end_train\n        end_train = ini_train + qt_per_part_train\n        if i == qt_parts-1: end_train = None\n        parms.append(dict({'path_data': path_data, \n                           'df_meta_ini_part': df_meta_ini[ini_train:end_train].copy()}))\n\npool = Pool(processes = qt_threads)\nret = pool.map(util0.meta_construct, parms)\n\ndf_meta = pd.concat(ret, axis=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Modeling\n\n# hyper-parameters\nk1  = 0.00005      #kregularizer1\nb1  = 0.00001      #bregularizer1\nk2  = 0.00005      #kregularizer2\nb2  = 0.00001      #bregularizer2\ndo  = 0.12000      #drop-out rate\nlr  = 0.00005      #learning rate\n\nif use_base_model:\n    #partindo de um modelo pré-treinado\n    base_model = ConvNeXtBase(weights='imagenet', include_top=True, input_shape=(224, 224, 3))        \n    for layer in base_model.layers: layer.trainable = True\n            \ndef model4parms(k1,b1,k2,b2,do,lr):\n\n    kernsz = (3,3,3)\n    stride = (1,1,1)\n    pollsz = (2,2,2)\n    filters = 20\n\n    input_layer = layers.Input(shape=(3, img_sq_size, img_sq_size, 3))\n\n    if use_base_model:\n        print('adicionando modelo de base')\n        x = base_model(input_layer)\n\n    x = layers.Conv3D(filters=filters, kernel_size=kernsz, strides=stride, padding=\"same\",activation=\"relu\", \n                      kernel_initializer=initializers.he_normal(), \n                      kernel_regularizer=regularizers.L2(k1),\n                      bias_regularizer=regularizers.L2(b1))(input_layer)\n\n    layers.BatchNormalization()(x)\n\n    x = layers.MaxPooling3D(pool_size=pollsz, strides=pollsz, padding=\"same\")(x)\n\n    for _ in range(3):\n\n        filters *= 2\n\n        x = layers.Conv3D(filters=filters, kernel_size=kernsz, strides=stride, padding=\"same\", activation=\"relu\", \n                          kernel_initializer=initializers.he_normal(), \n                          kernel_regularizer=regularizers.L2(k1), \n                          bias_regularizer=regularizers.L2(b1))(x)\n\n        x = layers.BatchNormalization()(x)\n\n        x = layers.MaxPooling3D(pool_size=pollsz, strides=pollsz, padding=\"same\")(x)\n\n    x = layers.Flatten()(x)\n\n    x = layers.Dense(activation='relu', units=333, \n                     kernel_regularizer=regularizers.L2(k2), \n                     bias_regularizer=regularizers.L2(b2))(x)\n\n    x = layers.Dropout(do)(x)\n\n    x = layers.Dense(activation='relu', units=33, \n                     kernel_regularizer=regularizers.L2(k2), \n                     bias_regularizer=regularizers.L2(b2))(x)\n\n    x = layers.Dropout(do)(x)\n\n    outputs = [layers.Dense(2, activation='softmax', name=f'output{i}')(x) for i in range(qt_categs)]\n    concatenated = layers.Concatenate(axis=1)(outputs)\n    model = models.Model(inputs=input_layer, outputs=concatenated)\n\n    optimizer1 = Adam(learning_rate=lr)\n\n    model.compile(optimizer=optimizer1, loss=['categorical_crossentropy']*3, metrics='mean_absolute_error')\n\n    return model\n\n# Model instantiation\nmodel_img = model4parms(k1,b1,k2,b2,do,lr)\nprint(model_img.summary())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sets de treinamento, validação e holdout\n\n# Separação dos sets por categoria (label) para fins de balanceamento \ndf_meta_train_per_categ = []\ndf_meta_valid_per_categ = []\ndf_meta_tstng_per_categ = []\ndf_meta['categ'] = -1\n\nfor i in range(qt_categs+1):\n    if i==0: qry=\"Miner==0 and Rust==0 and Phoma==0\" #Sem doença\n    if i==1: qry=\"Miner==1 and Rust==0 and Phoma==0\" #Miner\n    if i==2: qry=\"Miner==0 and Rust==1 and Phoma==0\" #Rust\n    if i==3: qry=\"Miner==0 and Rust==0 and Phoma==1\" #Phoma\n    df_categ = df_meta.query(qry).copy()\n    df_categ['categ'] = i\n\n    #Separa 60% para treinamento. Do restante, 60% para validação. Restante para holdout.\n    df_train = df_categ.sample(frac=.6, replace=False)\n    id_remng = [x for x in df_categ['ImgName'].values if x not in df_train['ImgName'].values]\n    df_valid = df_categ[df_categ['ImgName'].isin(id_remng)].sample(frac=.6, replace=False)\n    id_remng = [x for x in df_categ['ImgName'].values if x not in df_train['ImgName'].values and x not in df_valid['ImgName'].values]\n    df_tstng = df_categ[df_categ['ImgName'].isin(id_remng)]\n    df_meta_train_per_categ.append(df_train)\n    df_meta_valid_per_categ.append(df_valid)\n    df_meta_tstng_per_categ.append(df_tstng)\n\n# Salva datasets para eventual continuacao de treinamento\ndf_meta_train = pd.concat(df_meta_train_per_categ, axis=0)\ndf_meta_train.to_csv('df_meta_train.csv')\ndf_meta_valid = pd.concat(df_meta_valid_per_categ, axis=0)\ndf_meta_valid.to_csv('df_meta_valid.csv')\ndf_meta_tstng = pd.concat(df_meta_tstng_per_categ, axis=0)\ndf_meta_tstng.to_csv('df_meta_tstng.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Epochs de treinmanto\n\nbest_mea =  +9999\n\nfor h in range(epochs_traininig):\n\n    # Sampling\n    print('Sampling #', h)\n\n    df_meta_train_smp = []\n    for i in range(qt_categs+1):\n        df_meta_train_smp.append(df_meta_train_per_categ[i].sample(n=smp_sz, replace=True))\n\n    df_meta_valid_smp = []\n    for i in range(qt_categs+1):\n        df_meta_valid_smp.append(df_meta_valid_per_categ[i].sample(n=smp_sz, replace=True))\n\n    df_meta_train_smp = pd.concat(df_meta_train_smp, axis=0).sample(frac=1, replace=False)\n    df_meta_valid_smp = pd.concat(df_meta_valid_smp, axis=0).sample(frac=1, replace=False)\n\n    # Preparação das imagens\n    print('Preparação das imagens')\n\n    parms = []\n    end_train = 0\n    end_valid = 0\n\n    qt_per_part_train = int(df_meta_train_smp.shape[0] / qt_parts)\n    qt_per_part_valid = int(df_meta_valid_smp.shape[0] / qt_parts)\n\n    for i in range(qt_parts):\n\n            ini_train = end_train\n            end_train = ini_train + qt_per_part_train\n\n            ini_valid = end_valid\n            end_valid = ini_valid + qt_per_part_valid\n\n            if i == qt_parts-1:\n                end_train = end_valid = None\n\n            parms.append(dict({'df_meta_smp':df_meta_train_smp[ini_train:end_train], \n                               'path_data':path_data, 'setx':'train', 'file_ext':file_ext, \n                               'rodada':i, 'img_sq_size':img_sq_size, 'imgnm_field':imgnm_field,\n                               'imdir_field':imdir_field,'lbl01_field':lbl01_field, 'aug':True, 'seg':False}))\n\n            parms.append(dict({'df_meta_smp':df_meta_valid_smp[ini_valid:end_valid], \n                               'path_data':path_data, 'setx':'valid', 'file_ext':file_ext, \n                               'rodada':i, 'img_sq_size':img_sq_size, 'imgnm_field':imgnm_field,\n                               'imdir_field':imdir_field,'lbl01_field':lbl01_field, 'aug':True, 'seg':False}))\n\n    pool = Pool(processes = qt_threads)\n    ret = pool.map(util1f.smp2vol, parms)\n\n    # Descompacta resultados\n    print('Descompacta resultados')\n\n    for i in range(qt_parts *1):\n        ret[i][0] = pickle.loads(zlib.decompress(ret[i][0]))\n\n    Xt = np.vstack([ret[i][0] for i in range(len(ret)) if ret[i][2]=='train'])\n    Xv = np.vstack([ret[i][0] for i in range(len(ret)) if ret[i][2]=='valid'])\n\n    # Prepara labels para softmax\n    print('Prepara labels para softmax')\n\n    Yt = []\n    Yv = []\n    for i in range(len(ret)):\n        if ret[i][2]=='train': Yt = Yt + ret[i][1] \n        if ret[i][2]=='valid': Yv = Yv + ret[i][1] \n    Yt = np.array(Yt)\n    Yv = np.array(Yv)\n\n    #Treina\n    print('Treina')\n\n    stt = model_img.fit(Xt, Yt, epochs=1, batch_size=1,\n                validation_data=(Xv, Yv), verbose=2, shuffle=False)\n\n    mea = stt.history.get(\"val_mean_absolute_error\")[0]\n\n    if mea < best_mea:\n        best_mea = mea\n        qtd_saved +=1\n        model_img.save('best_model.h5')\n        print('best measure', mea)\n\n    #limpa memoria\n    gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"raw","source":"print('ok')","metadata":{"execution":{"iopub.execute_input":"2023-11-02T16:42:56.220288Z","iopub.status.busy":"2023-11-02T16:42:56.219602Z","iopub.status.idle":"2023-11-02T16:42:56.229008Z","shell.execute_reply":"2023-11-02T16:42:56.227990Z","shell.execute_reply.started":"2023-11-02T16:42:56.220253Z"}}}]}